{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd3f17ba",
   "metadata": {},
   "source": [
    "***Lab 3***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed tools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dacb41c",
   "metadata": {},
   "source": [
    "**Generate random numbers 𝛼 ∈ (0,1) and 𝛽 ∈ (0,1). Furthermore, generate the noisy data {𝑥_𝑘, 𝑦_𝑘}, where 𝑘 = 0,...,100, according to the following rule:**\n",
    "\n",
    "**𝑦_𝑘 = 𝛼𝑥_𝑘 + 𝛽 + 𝛿_𝑘, 𝑥_𝑘 = 𝑘/100,**\n",
    "\n",
    "**where 𝛿_𝑘 ~ 𝑁(0,1) are values of a random variable with standard normal distribution. Approximate the data by the following linear and rational functions:**\n",
    "\n",
    "**1. 𝐹(𝑥,𝑎,𝑏) = 𝑎𝑥 + 𝑏 (linear approximant),**\n",
    "\n",
    "**2. 𝐹(𝑥,𝑎,𝑏) = 𝑎/(1+𝑏𝑥) (rational approximant),**\n",
    "\n",
    "**by means of least squares through the numerical minimization (with precision 𝜀 = 0.001) of the following function:**\n",
    "\n",
    "**𝐷(𝑎,𝑏) = sum(𝑘=0, 100)((𝐹(𝑥_𝑘,𝑎,𝑏) − 𝑦_𝑘)^2).**\n",
    "\n",
    "**To solve the minimization problem, use the methods of Gradient Descent, Conjugate Gradient Descent, Newton’s method and Levenberg-Marquardt algorithm. If necessary, set the initial approximations and other parameters of the methods. Visualize the data and the approximants obtained in a plot separately for each type of approximant so that one can compare the results for the numerical methods used. Analyze the results obtained (in terms of number of iterations, precision, number of function evaluations, etc.) and compare them with those from Task 2 for the same dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9e3a84",
   "metadata": {},
   "source": [
    "    0. Generating needed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb8d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random numbers alpha and beta\n",
    "alpha = np.random.uniform(0,1)\n",
    "beta = np.random.uniform(0,1)\n",
    "#Noisy data\n",
    "x_k = np.array([k/100 for k in range(0,101)])\n",
    "y_k = [alpha * k + beta + np.random.normal(0.5,0.125) for k in x_k]\n",
    "\n",
    "plt.plot(x_k, y_k, '.b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3172ed15",
   "metadata": {},
   "source": [
    "    1.0. Methods from Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4634f2",
   "metadata": {},
   "source": [
    "        1.0.1. For linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brute-force method\n",
    "lin_ans = []\n",
    "for a in range(0,1001):\n",
    "    a /= 1000\n",
    "    for b in range(0,1001):\n",
    "        b /= 1000\n",
    "        dot_number = 0\n",
    "        for k in range(0,101):\n",
    "            dot_number += (a * x_k[k] + b - y_k[k]) ** 2\n",
    "        lin_ans.append([dot_number,a,b])\n",
    "\n",
    "minimum = min(i[0] for i in lin_ans)\n",
    "for elem in lin_ans:\n",
    "    if elem[0]==minimum:\n",
    "        a = elem[1]\n",
    "        b = elem[2]\n",
    "        break\n",
    "\n",
    "brute_force_lin = [a,b]\n",
    "\n",
    "#Gauss method\n",
    "a1 = 0.05\n",
    "b1 = 0.05\n",
    "b1_temp = 10000\n",
    "number_of_iterations = 0\n",
    "number_of_dots = 0\n",
    "while True:\n",
    "    number_of_iterations += 2\n",
    "    ans_a = []\n",
    "    for a in range(0, 1001):\n",
    "        number_of_dots += 1\n",
    "        a /= 1000\n",
    "        dot_number = 0\n",
    "        for k in range(0, 101):\n",
    "            dot_number += (a * x_k[k] + b1 - y_k[k]) ** 2\n",
    "        ans_a.append(dot_number)\n",
    "    a1_temp = ans_a.index(min(ans_a)) / 1000\n",
    "    if abs(a1 - a1_temp) < 0.001 and abs(b1 - b1_temp) < 0.001:\n",
    "        break\n",
    "    a1 = a1_temp\n",
    "\n",
    "    ans_b = []\n",
    "    for b in range(0, 1001):\n",
    "        number_of_dots += 1\n",
    "        b/=1000\n",
    "        dot_number = 0\n",
    "        for k in range(0, 101):\n",
    "            dot_number += (a1 * x_k[k] + b - y_k[k]) ** 2\n",
    "        ans_b.append(dot_number)\n",
    "    b1_temp = ans_b.index(min(ans_b)) / 1000\n",
    "    if abs(b1 - b1_temp) < 0.001 and abs(a1 - a1_temp) < 0.001:\n",
    "        break\n",
    "    b1 = b1_temp\n",
    "\n",
    "gauss_lin = [a1,b1]\n",
    "\n",
    "#Nelder-Mead method\n",
    "def lin_func(data):\n",
    "    a,b = data\n",
    "    ans = 0\n",
    "    for k in range(0,101):\n",
    "        ans += (a * x_k[k] + b - y_k[k]) ** 2\n",
    "    return ans\n",
    "\n",
    "res = minimize(lin_func, [0.3, 0.3], method = 'nelder-mead', options = {'xatol' : 0.001, 'disp' : True})\n",
    "nm_lin = res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4141671b",
   "metadata": {},
   "source": [
    "        1.0.2. For rational function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brute-force method\n",
    "rat_ans = []\n",
    "for a in range(0,1001):\n",
    "    a /= 1000\n",
    "    for b in range(0,1001):\n",
    "        b = -b / 1000\n",
    "        dot_number = 0\n",
    "        for k in range(0,101):\n",
    "            dot_number += (a / (1 + b * x_k[k]) - y_k[k]) ** 2\n",
    "        rat_ans.append([dot_number,a,b])\n",
    "\n",
    "minimum = min(i[0] for i in rat_ans)\n",
    "for elem in rat_ans:\n",
    "    if elem[0]==minimum:\n",
    "        a = elem[1]\n",
    "        b = elem[2]\n",
    "        break\n",
    "\n",
    "brute_force_rat = [a,b]\n",
    "\n",
    "#Gauss method\n",
    "a1 = 0.05\n",
    "b1 = -0.05\n",
    "b1_temp = 10000\n",
    "number_of_iterations = 0\n",
    "number_of_dots = 0\n",
    "while True:\n",
    "    number_of_iterations += 2\n",
    "    ans_a = []\n",
    "    for a in range(0, 1000):\n",
    "        number_of_dots += 1\n",
    "        a /= 1000\n",
    "        dot_number = 0\n",
    "        for k in range(0, 101):\n",
    "            dot_number += (a / (1 + b1 * x_k[k]) - y_k[k]) ** 2\n",
    "        ans_a.append(dot_number)\n",
    "    a1_temp = ans_a.index(min(ans_a)) / 1000\n",
    "    if abs(a1 - a1_temp) < 0.001 and abs(b1 - b1_temp) < 0.001:\n",
    "        break\n",
    "    a1 = a1_temp\n",
    "\n",
    "    ans_b = []\n",
    "    for b in range(0, 1000):\n",
    "        number_of_dots += 1\n",
    "        b = -b/1000\n",
    "        dot_number = 0\n",
    "        for k in range(0, 101):\n",
    "            dot_number += (a1 / (1 + b * x_k[k]) - y_k[k])**2\n",
    "        ans_b.append(dot_number)\n",
    "    b1_temp = ans_b.index(min(ans_b)) / -1000\n",
    "    if abs(b1 - b1_temp) < 0.001 and abs(a1 - a1_temp) < 0.001:\n",
    "        break\n",
    "    b1 = b1_temp\n",
    "    \n",
    "gauss_rat = [a1,b1]\n",
    "\n",
    "#Nelder-Mead method\n",
    "def rat_func(data):\n",
    "    a,b = data\n",
    "    ans = 0\n",
    "    for k in range(0,101):\n",
    "        ans += (a / (1 + b * x_k[k]) - y_k[k]) ** 2\n",
    "    return ans\n",
    "\n",
    "res = minimize(rat_func, [0.3, 0.3], method = 'nelder-mead', options = {'xatol' : 0.001, 'disp' : True})\n",
    "nm_rat = res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23dc69",
   "metadata": {},
   "source": [
    "    1.1. Gradient descent method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d376ce",
   "metadata": {},
   "source": [
    "        1.1.1. For linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ff653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x_k, y_k, w, b, lr, func_type):\n",
    "    d1dw = 0.0\n",
    "    d1db = 0.0\n",
    "    N= x_k.shape[0]\n",
    "    \n",
    "    for x_i, y_i in zip(x_k, y_k):\n",
    "        d1dw += 2 * (w * x_i + b - y_i) * x_i\n",
    "        d1db += 2 * (w * x_i + b - y_i)\n",
    "    \n",
    "    w = w - lr * (1 / N) * d1dw\n",
    "    b = b - lr * (1 / N) * d1db\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f060b",
   "metadata": {},
   "source": [
    "        1.1.2. For rational function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rational(x_k, y_k, w, b, lr, func_type):\n",
    "    d1dw = 0.0\n",
    "    d1db = 0.0\n",
    "    N= x_k.shape[0]\n",
    "    for x_i, y_i in zip(x_k, y_k):\n",
    "        d1dw += 2 * (w / (1 + b * x_i) - y_i) / (1 + b * x_i)\n",
    "        d1db += 2 * (w / (1 + b * x_i) - y_i) * (- x_i * w / (1 + b * x_i) ** 2)\n",
    "    \n",
    "    w = w - lr * (1 / N) * d1dw\n",
    "    b = b - lr * (1 / N) * d1db\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da663405",
   "metadata": {},
   "source": [
    "        1.1.3. Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bede5cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_k, y_k, num_epoch, lr, func_type):  \n",
    "    w, b = 0.0, 0.0\n",
    "    for epoch in range(num_epoch):\n",
    "        if func_type == 'linear':\n",
    "            w, b = linear(x_k, y_k, w, b, lr, func_type)\n",
    "            yhat = w * x_k + b\n",
    "            loss = np.divide(np.sum((yhat - y_k) ** 2, axis=0), x_k.shape[0])\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'epoch: {epoch}, loss: {round(loss, 4)}, true: {round(alpha, 4), round(beta, 4)}, curr: {round(w, 4), round(b, 4)}')\n",
    "        else:\n",
    "            w, b = rational(x_k, y_k, w, b, lr, func_type)\n",
    "            yhat = w / (1 + b * x_k)\n",
    "            loss = np.divide(np.sum((yhat - y_k) ** 2, axis=0), x_k.shape[0])\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'epoch: {epoch}, loss: {round(loss, 4)}, true: {round(alpha, 4), round(beta, 4)}, curr: {round(w, 4), round(b, 4)}')\n",
    "    return w, b\n",
    "\n",
    "num_epochs = 5000\n",
    "lr = 0.01\n",
    "eps = 0.001\n",
    "func_types = ['linear', 'rational']\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize = (10, 8))\n",
    "for i, func in enumerate(func_types):\n",
    "    print(f'Start training for {func}')\n",
    "    w, b = train(x_k, y_k, num_epochs, lr, func)\n",
    "    \n",
    "    axs[i].plot(x_k, y_k, '.b')\n",
    "    if func == 'linear':\n",
    "        GD_linear = [w, b]\n",
    "        axs[i].plot(x_k, w * x_k + b, 'r')\n",
    "    else:\n",
    "        GD_rat = [w, b]\n",
    "        axs[i].plot(x_k, w / (1 + b * x_k), 'r') \n",
    "    axs[i].set_title(f'{func} approximation')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4dff0b",
   "metadata": {},
   "source": [
    "    1.2. Conjugate Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0fca44",
   "metadata": {},
   "source": [
    "        1.2.1. For linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf214ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(wb):\n",
    "    w, b = wb\n",
    "    return np.sum((w * x_k + b - y_k) ** 2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72991069",
   "metadata": {},
   "source": [
    "        1.2.2. For rational function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7472514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rational(wb):\n",
    "    w, b = wb\n",
    "    return np.sum((w / (1 + b * x_k) - y_k) ** 2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18456e82",
   "metadata": {},
   "source": [
    "        1.2.3. Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd474ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_types = [linear, rational]\n",
    "start_values = [[1., 1.], [1., -0.5]]\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize = (10, 8))\n",
    "for i, (func, start) in enumerate(zip(func_types, start_values)):\n",
    "    print(f'Start training for {func}')\n",
    "    CGD = minimize(func, start, method = 'CG', options = {'xtol' : 1e-3, 'disp' : True})\n",
    "    w, b = CGD.x\n",
    "    axs[i].plot(x_k, y_k, '.b')\n",
    "    if func == linear:\n",
    "        CGD_linear = CGD.x\n",
    "        axs[i].plot(x_k, w * x_k + b, 'r')\n",
    "    else:\n",
    "        CGD_rat = CGD.x\n",
    "        axs[i].plot(x_k, w / (1 + b * x_k), 'r') \n",
    "    axs[i].set_title(f'{func} approximation')\n",
    "    print(f'Founded values: {w, b}, real values: {alpha, beta}')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904307b",
   "metadata": {},
   "source": [
    "    1.3. Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765f9a0",
   "metadata": {},
   "source": [
    "        1.3.1. For linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(wb):\n",
    "    w, b = wb\n",
    "    return np.sum((w * x_k + b - y_k) ** 2, axis=0)\n",
    "\n",
    "def d_linear(wb):\n",
    "    w, b = wb\n",
    "    return np.array([np.sum(2 * x_k * (b + w * x_k - y_k)), np.sum(2 * (b + w * x_k - y_k))])\n",
    "\n",
    "def hess_lin(wb):\n",
    "    w, b = wb\n",
    "    hess = np.ones([2,2])\n",
    "    hess[0,0] = np.sum(2 * x_k ** 2)\n",
    "    hess[0,1] = np.sum(2 * x_k)\n",
    "    hess[1,0] = np.sum(2 * x_k)\n",
    "    hess[1,1] = (2)\n",
    "    return hess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952a844",
   "metadata": {},
   "source": [
    "        1.3.2. For rational function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbaddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rational(wb):\n",
    "    w, b = wb\n",
    "    return np.sum((w / (1 + b * x_k) - y_k) ** 2, axis=0)\n",
    "\n",
    "def d_rational(wb):\n",
    "    w, b = wb\n",
    "    return np.array([np.sum((w / (1 + b * x_k) - y_k) * 2 / (1 + b * x_k)), np.sum(2 * w * x_k / (1 + b * x_k) ** 2 * (w / (1 + b * x_k) - y_k))])\n",
    "\n",
    "def hess_rat(wb):\n",
    "    w, b = wb\n",
    "    hess = np.ones([2,2])\n",
    "    hess[0,0] = np.sum(2 / (1 + b * x_k) ** 2)\n",
    "    hess[0,1] = np.sum(-2 * w * x_k / (1 + b * x_k) ** 3 - 2 * x_k * (w / (1 + b * x_k) - y_k) / (1 + b * x_k) ** 3)\n",
    "    hess[1,0] = np.sum(-2 * w * x_k / (1 + b * x_k) ** 3 - 2 * x_k * (w / (1 + b * x_k) - y_k) / (1 + b * x_k) ** 3)\n",
    "    hess[1,1] = np.sum(2 * w ** 2 * x_k ** 2 / (1 + b * x_k) **4 * 4 * w * x_k ** 2 * (w / (1 + b * x_k) - y_k) / (1 + b * x_k) ** 3)\n",
    "    return hess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0b420",
   "metadata": {},
   "source": [
    "        1.3.3. Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317fd677",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_types = [linear, rational]\n",
    "jac_types = [d_linear, d_rational]\n",
    "hess_types = [hess_lin, hess_rat]\n",
    "start_values = [[1.0, 1.0], [1.1, -0.5]]\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize = (10, 8))\n",
    "for i, (func, jac, hess, start) in enumerate(zip(func_types, \n",
    "                                          jac_types, \n",
    "                                          hess_types,\n",
    "                                          start_values)):\n",
    "    print(f'Start training for {func}')\n",
    "    newton = minimize(func, start, method = 'Newton-CG', jac = jac, hess = hess, options = {'xtol' : 1e-3, 'disp' : True})\n",
    "    w, b = newton.x\n",
    "    \n",
    "    axs[i].plot(x_k, y_k, '.b')\n",
    "    if func == linear:\n",
    "        Newton_linear = newton.x\n",
    "        axs[i].plot(x_k, w * x_k + b, 'r')\n",
    "    else:\n",
    "        Newton_rat = newton.x\n",
    "        axs[i].plot(x_k, w / (1 + b * x_k), 'r')\n",
    "    axs[i].set_title(f'{func} approximation')\n",
    "    print(f'Founded values: {w, b}, real values: {alpha, beta}')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec4742",
   "metadata": {},
   "source": [
    "    1.4. Levenberg-Marquardt method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d80d98",
   "metadata": {},
   "source": [
    "        1.4.1. For linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60345e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(wb):\n",
    "    w, b = wb\n",
    "    return (w * x_k + b - y_k) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad34d4",
   "metadata": {},
   "source": [
    "        1.4.2. For rational function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379df9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rational(wb):\n",
    "    w, b = wb\n",
    "    return (w / (1 + b * x_k) - y_k) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289205a7",
   "metadata": {},
   "source": [
    "        1.4.3. Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_types = [linear, rational]\n",
    "start_values = [[1., 1.], [1., -0.5]]\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize = (10, 8))\n",
    "for i, (func, start) in enumerate(zip(func_types, start_values)):\n",
    "    print(f'Start training for {func}')\n",
    "    lma = optimize.least_squares(func, start, method = 'lm', xtol = 1e-3)\n",
    "    w, b = lma.x\n",
    "    \n",
    "    axs[i].plot(x_k, y_k, '.b')\n",
    "    if func == linear:\n",
    "        lma_linear = lma.x\n",
    "        axs[i].plot(x_k, w * x_k + b, 'r')\n",
    "    else:\n",
    "        lma_rat = lma.x\n",
    "        axs[i].plot(x_k, w / (1 + b * x_k), 'r') \n",
    "    axs[i].set_title(f'{func} approximation')\n",
    "    print(f'Founded values: {w, b}, real values: {alpha, beta}')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf07ca",
   "metadata": {},
   "source": [
    "    1.5. Comparison of methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b43df6a",
   "metadata": {},
   "source": [
    "        1.5.1. For linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3652151",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title('For linear function', fontsize=14)\n",
    "plt.plot(x_k, y_k, '.b', label='Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.plot(x_k, GD_linear[0] * x_k + GD_linear[1], 'g', label='Gradient Descent Method', linewidth=3)\n",
    "plt.plot(x_k, CGD_linear[0] * x_k + CGD_linear[1], 'r', label='Conjugate Gradient Descent Method', linewidth=3)\n",
    "plt.plot(x_k, Newton_linear[0] * x_k + Newton_linear[1], 'y', label='Newton Method', linewidth=3)\n",
    "plt.plot(x_k, lma_linear[0] * x_k + lma_linear[1], 'm', label='Levenberg-Marquardt Method', linewidth=3)\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391ea0cc",
   "metadata": {},
   "source": [
    "        1.5.2. For rational function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa8624",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title('For rational function', fontsize=14)\n",
    "plt.plot(x_k, y_k, '.b', label='Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.plot(x_k, GD_rat[0] / (1 + GD_rat[1] * x_k), 'g', label='Gradient Descent Method', linewidth=3)\n",
    "plt.plot(x_k, CGD_rat[0] / (1 + CGD_rat[1] * x_k), 'r', label='Conjugate Gradient Descent Method', linewidth=3)\n",
    "plt.plot(x_k, Newton_rat[0] / (1 + Newton_rat[1] * x_k), 'y', label='Newton Method', linewidth=3)\n",
    "plt.plot(x_k, lma_rat[0] / (1 + lma_rat[1] * x_k), 'm', label='Levenberg-Marquardt Method', linewidth=3)\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b287b",
   "metadata": {},
   "source": [
    "        1.5.3. For linear function with methods from Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title('For linear function', fontsize=14)\n",
    "plt.plot(xk, yk, '.b', label='Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.plot(x_k, brute_force_lin[0] * x_k + brute_force_lin[1], 'g', label = 'Brute-Force Method', linewidth = 3)\n",
    "plt.plot(x_k, gauss_lin[0] * x_k + gauss_lin[1], 'y', label = 'Gauss Method', linewidth = 3)\n",
    "plt.plot(x_k, nm_lin[0] * x_k + nm_lin[1], 'r', label = 'Nelder-Mead Method', linewidth = 3)\n",
    "plt.plot(x_k, GD_linear[0] * x_k + GD_linear[1], 'g', label='Gradient Descent Method', linewidth=3)\n",
    "plt.plot(x_k, CGD_linear[0] * x_k + CGD_linear[1], 'r', label='Conjugate Gradient Descent Method', linewidth=3)\n",
    "plt.plot(x_k, Newton_linear[0] * x_k + Newton_linear[1], 'y', label='Newton Method', linewidth=3)\n",
    "plt.plot(x_k, lma_linear[0] * x_k + lma_linear[1], 'm', label='Levenberg-Marquardt Method', linewidth=3)\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a2ac1",
   "metadata": {},
   "source": [
    "        1.5.4. For rational function with methods from Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f41169",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title('For rational function', fontsize=14)\n",
    "plt.plot(xk, yk, '.b', label='Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.plot(x_k, brute_force_rat[0] / (1 + brute_force_rat[1] * x_k), 'g', label = 'Brute-Force Method', linewidth = 3)\n",
    "plt.plot(x_k, gauss_rat[0] / (1 + gauss_rat[1] * x_k), 'y', label = 'Gauss Method', linewidth = 3)\n",
    "plt.plot(x_k, nm_rat[0] / (1 + nm_rat[1] * x_k), 'r', label = 'Nelder-Mead Method', linewidth = 3)\n",
    "plt.plot(xk, GD_rat[0] / (1 + GD_rat[1] * x_k), 'g', label='Gradient Descent Method', linewidth=3)\n",
    "plt.plot(xk, CGD_rat[0] /(1 + CGD_rat[1] * x_k), 'r', label='Conjugate Gradient Descent Method', linewidth=3)\n",
    "plt.plot(xk, Newton_rat[0] / (1 + Newton_rat[1] * x_k), 'y', label='Newton Method', linewidth=3)\n",
    "plt.plot(xk, lma_rat[0] / (1 + lma_rat[1] * x_k), 'm', label='Levenberg-Marquardt Method', linewidth=3)\n",
    "plt.legend(fontsize=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
